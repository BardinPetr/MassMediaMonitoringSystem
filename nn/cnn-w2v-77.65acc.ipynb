{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1_77 (2).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq0mvproeGPZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59d75ec9-532a-4ff5-cd5f-15c28bad7cf6"
      },
      "source": [
        "!rm *.csv\n",
        "!wget http://smartheatmap.ddns.net/dist/p.csv\n",
        "!wget http://smartheatmap.ddns.net/dist/n.csv\n",
        "!pip3 install wldhx.yadisk-direct hyperas hyperopt\n",
        "!curl -L $(yadisk-direct https://yadi.sk/d/NmzmzI1_v9tecQ) -o w2v.zip\n",
        "!unzip w2v.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '*.csv': No such file or directory\n",
            "--2019-07-17 12:40:01--  http://smartheatmap.ddns.net/dist/p.csv\n",
            "Resolving smartheatmap.ddns.net (smartheatmap.ddns.net)... 188.120.231.51\n",
            "Connecting to smartheatmap.ddns.net (smartheatmap.ddns.net)|188.120.231.51|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27630235 (26M) [text/csv]\n",
            "Saving to: ‘p.csv’\n",
            "\n",
            "p.csv               100%[===================>]  26.35M  17.6MB/s    in 1.5s    \n",
            "\n",
            "2019-07-17 12:40:02 (17.6 MB/s) - ‘p.csv’ saved [27630235/27630235]\n",
            "\n",
            "--2019-07-17 12:40:04--  http://smartheatmap.ddns.net/dist/n.csv\n",
            "Resolving smartheatmap.ddns.net (smartheatmap.ddns.net)... 188.120.231.51\n",
            "Connecting to smartheatmap.ddns.net (smartheatmap.ddns.net)|188.120.231.51|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26037206 (25M) [text/csv]\n",
            "Saving to: ‘n.csv’\n",
            "\n",
            "n.csv               100%[===================>]  24.83M  18.2MB/s    in 1.4s    \n",
            "\n",
            "2019-07-17 12:40:05 (18.2 MB/s) - ‘n.csv’ saved [26037206/26037206]\n",
            "\n",
            "Collecting wldhx.yadisk-direct\n",
            "  Downloading https://files.pythonhosted.org/packages/20/a9/e14d8abec847b839d95d131dede68aa03c0bd3ca6485cca406550dfa2a3a/wldhx.yadisk_direct-0.0.6-py3-none-any.whl\n",
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/04/34/87ad6ffb42df9c1fa9c4c906f65813d42ad70d68c66af4ffff048c228cd4/hyperas-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from wldhx.yadisk-direct) (2.21.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.4)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.16.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.8.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->wldhx.yadisk-direct) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->wldhx.yadisk-direct) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->wldhx.yadisk-direct) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->wldhx.yadisk-direct) (2019.6.16)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.5.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (6.0.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.5.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (4.5.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (4.3.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10.1)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.1.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.0)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (5.3.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (4.5.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->hyperas) (5.5.0)\n",
            "Collecting prompt-toolkit<2.1.0,>=2.0.0 (from jupyter-console->jupyter->hyperas)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/9b1dd14ef45345f186ef69d175bdd2491c40ab1dfa4b2b3e4352df719ed7/prompt_toolkit-2.0.9-py3-none-any.whl (337kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter->hyperas) (2.5.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter->hyperas) (17.0.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (41.0.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (4.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
            "\u001b[31mERROR: ipython 5.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.4, but you'll have prompt-toolkit 2.0.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: wldhx.yadisk-direct, hyperas, prompt-toolkit\n",
            "  Found existing installation: prompt-toolkit 1.0.16\n",
            "    Uninstalling prompt-toolkit-1.0.16:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.16\n",
            "Successfully installed hyperas-0.4.1 prompt-toolkit-2.0.9 wldhx.yadisk-direct-0.0.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1478M  100 1478M    0     0  44.3M      0  0:00:33  0:00:33 --:--:-- 48.1M\n",
            "Archive:  w2v.zip\n",
            "   creating: w2v/\n",
            "  inflating: w2v/tweets_model.w2v    \n",
            "  inflating: w2v/tweets_model.w2v.trainables.syn1neg.npy  \n",
            "  inflating: w2v/tweets_model.w2v.wv.vectors.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KX9RZbbe4Rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_positive = pd.read_csv('p.csv', sep=';', error_bad_lines=False, usecols=['ttext'])\n",
        "data_negative = pd.read_csv('n.csv', sep=';', error_bad_lines=False, usecols=['ttext'])\n",
        "\n",
        "sample_size = min(data_positive.shape[0], data_negative.shape[0])\n",
        "raw_data = np.concatenate((data_positive['ttext'].values[:sample_size],\n",
        "                           data_negative['ttext'].values[:sample_size]), axis=0)\n",
        "labels = [1] * sample_size + [0] * sample_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdwh85CUfWVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def clear_text(text):\n",
        "    text = text.lower().replace(\"ё\", \"е\")\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n",
        "    text = re.sub('@[^\\s]+', 'USER', text)\n",
        "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "data = [clear_text(t) for t in raw_data]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftfxJbGafvG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvrTTX8qsHPc",
        "colab_type": "code",
        "outputId": "8be3c220-e3ae-4e0f-f6c1-e4e50882c82f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "SENTENCE_LENGTH = 26\n",
        "NUM = 100000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=NUM)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "def to_seq(x):\n",
        "    sequences = tokenizer.texts_to_sequences(x)\n",
        "    return pad_sequences(sequences, maxlen=SENTENCE_LENGTH)\n",
        "\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "  pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  \n",
        "x_train = to_seq(x_train)\n",
        "x_test = to_seq(x_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpaRSja3vSQi",
        "colab_type": "code",
        "outputId": "e1833d0f-1977-4b72-b6a1-24d052ed9724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "wvmodel = Word2Vec.load(\"w2v/tweets_model.w2v\")\n",
        "\n",
        "DIM = wvmodel.vector_size \n",
        "embedding_matrix = np.zeros((NUM, DIM))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= NUM:\n",
        "        break\n",
        "    if word in wvmodel.wv.vocab.keys():\n",
        "        embedding_matrix[i] = wvmodel.wv[word]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQtEgsvesVmW",
        "colab_type": "code",
        "outputId": "952f14d6-22c2-4565-d225-c254e9e0ab4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "from keras.layers import Input\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "m_input = Input(shape=(SENTENCE_LENGTH,), dtype='int32')\n",
        "# m_embed = Embedding(NUM, \n",
        "#                     DIM, \n",
        "#                     input_length=SENTENCE_LENGTH,\n",
        "#                     weights=[embedding_matrix], \n",
        "#                     trainable=True)(m_input)\n",
        "DIM=300\n",
        "m_embed = Embedding(NUM, \n",
        "                    DIM, \n",
        "                    input_length=SENTENCE_LENGTH)(m_input)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0717 12:41:42.540471 139626235393920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0717 12:41:42.584094 139626235393920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0717 12:41:42.594195 139626235393920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPYFH7Dbv68i",
        "colab_type": "code",
        "outputId": "5dfc1099-43bd-4d53-adfa-02cf50bdf0a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from keras import optimizers\n",
        "from keras.layers import Dense, concatenate, Activation, Dropout\n",
        "from keras.models import Model\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.pooling import GlobalMaxPooling1D\n",
        "\n",
        "x = Dropout(0.2)(m_embed)\n",
        "\n",
        "conv_branches = []\n",
        "for size, filters_count in [(2, 10), (3, 10), (4, 10), (5, 10)]:\n",
        "    for i in range(filters_count):\n",
        "        i = Conv1D(filters=1, \n",
        "                   kernel_size=size, \n",
        "                   padding='valid', \n",
        "                   activation='relu')(x)\n",
        "        i = GlobalMaxPooling1D()(i)\n",
        "        conv_branches.append(i)\n",
        "\n",
        "x = concatenate(conv_branches, \n",
        "                axis=1)\n",
        "\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(30, activation='relu')(x)\n",
        "x = Dense(1)(x)\n",
        "\n",
        "m_output = Activation('sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=[m_input], \n",
        "              outputs=[m_output])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0717 12:41:42.639526 139626235393920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0717 12:41:42.662533 139626235393920 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca_OIR_F85pL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "        \n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4YngqaPx8aH",
        "colab_type": "code",
        "outputId": "33a4c5ce-c663-4d82-bede-b81457afb456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy',\n",
        "                       precision, \n",
        "                       recall, \n",
        "                       f1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0717 12:41:43.842788 139626235393920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0717 12:41:43.874917 139626235393920 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0717 12:41:43.885639 139626235393920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRXnAy-avLxd",
        "colab_type": "code",
        "outputId": "195518bb-9168-4db5-8afd-d22db417ba4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "rm cnn*"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'cnn*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqV2fGm_yGFF",
        "colab_type": "code",
        "outputId": "b8a61745-ea0c-49cb-90d9-dbc503293196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"cnn-{epoch:02d}-{val_f1:.2f}.hdf5\", \n",
        "                             monitor='val_f1', \n",
        "                             save_best_only=True, \n",
        "                             mode='max', \n",
        "                             period=1)\n",
        "\n",
        "history = model.fit(x_train, \n",
        "                    y_train, \n",
        "                    batch_size=32, \n",
        "                    epochs=10, \n",
        "                    validation_split=0.25, \n",
        "                    callbacks = [checkpoint])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 134307 samples, validate on 44769 samples\n",
            "Epoch 1/10\n",
            "134307/134307 [==============================] - 382s 3ms/step - loss: 0.4981 - acc: 0.7463 - precision: 0.7489 - recall: 0.7318 - f1: 0.7324 - val_loss: 0.4560 - val_acc: 0.7765 - val_precision: 0.7862 - val_recall: 0.7596 - val_f1: 0.7669\n",
            "Epoch 2/10\n",
            "134307/134307 [==============================] - 368s 3ms/step - loss: 0.3415 - acc: 0.8502 - precision: 0.8555 - recall: 0.8426 - f1: 0.8445 - val_loss: 0.4634 - val_acc: 0.7769 - val_precision: 0.7915 - val_recall: 0.7527 - val_f1: 0.7657\n",
            "Epoch 3/10\n",
            "134307/134307 [==============================] - 371s 3ms/step - loss: 0.2087 - acc: 0.9152 - precision: 0.9208 - recall: 0.9084 - f1: 0.9119 - val_loss: 0.5597 - val_acc: 0.7707 - val_precision: 0.7796 - val_recall: 0.7553 - val_f1: 0.7611\n",
            "Epoch 4/10\n",
            "134307/134307 [==============================] - 369s 3ms/step - loss: 0.1306 - acc: 0.9490 - precision: 0.9534 - recall: 0.9442 - f1: 0.9471 - val_loss: 0.6858 - val_acc: 0.7621 - val_precision: 0.7775 - val_recall: 0.7334 - val_f1: 0.7486\n",
            "Epoch 5/10\n",
            "  3296/134307 [..............................] - ETA: 5:46 - loss: 0.0771 - acc: 0.9706 - precision: 0.9789 - recall: 0.9616 - f1: 0.9689"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "E0717 13:06:58.427597 139626235393920 ultratb.py:152] Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-12-3b7c340669eb>\", line 14, in <module>\n",
            "    callbacks = [checkpoint])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1039, in fit\n",
            "    validation_steps=validation_steps)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\", line 199, in fit_loop\n",
            "    outs = f(ins_batch)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2715, in __call__\n",
            "    return self._call(inputs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 2675, in _call\n",
            "    fetched = self._callable_fn(*array_vals)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1458, in __call__\n",
            "    run_metadata_ptr)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 742, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 395, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
            "    if not islink(newpath):\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 171, in islink\n",
            "    st = os.lstat(path)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0ueu6hRGTLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict(to_seq([\"Зачем мне подрывать и без того не самую безупречную репутацию кураторов?\",\n",
        "                       \"сломанная лавочка\",\n",
        "                       \"Ну это же логично, что я дождался, пока ты прочтешь и удалил\",\n",
        "                       \"трансгендер на улице\",\n",
        "                       \"США\",\n",
        "                       \"СССР\",\n",
        "                       \"фашист\",\n",
        "                       \"на улице поставили хреновую красивую скамейку\",\n",
        "                       \"на улице хрено поставили красивую скамейку\",\n",
        "                       \"на улице Ленина поставили хреновую скамейку\",\n",
        "                       \"красивую скамейку\",\n",
        "                       \"мне нравится эта хрень, но будет нереально круто\",\n",
        "                       \"Жить конечно тут наверное нет\",\n",
        "                       \"Очень-очень красиво, сказочно, солнечно, жарко, волшебно....можно много слов написать, и все они будут про это не забываемое место. Я  не знаю у кого как. Но я очень люблю Сочи-Адлер и люблю тут отдыхать с детьми и одна и с подругами и вообще. Жить конечно тут наверное нет. Но летом сюда-это точно. На все лето, сюда☺☺☺🤩🤩🤩🤩🤩\",\n",
        "                       \"эта скамейка очень красивая\",\n",
        "                       \"сегодня было грустно\",\n",
        "                       \"эта скамейка полная чушь\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}